{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import string\n",
    "\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [prompt, task, score_0, score_1, score_2, score_3, score_4, score_5, score_6, labels]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Define column names and data types\n",
    "columns = {\n",
    "    \"prompt\": \"string\",\n",
    "    \"task\": \"string\",  # Task name\n",
    "    \"score_0\": \"float\",  # Eurdem__Defne-llama3.1-8B\n",
    "    \"score_1\": \"float\",  # Locutusque__Hercules-6.1-Llama-3.1-8B\n",
    "    \"score_2\": \"float\",  # Nekochu__Llama-3.1-8B-German-ORPO\n",
    "    \"score_3\": \"float\",  # VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct\n",
    "    \"score_4\": \"float\",  # ValiantLabs__Llama3.1-8B-ShiningValiant2\n",
    "    \"score_5\": \"float\",  # arcee-ai__Llama-Spark\n",
    "    \"score_6\": \"float\",  # meta-llama__Llama-3.1-8B-Instruct\n",
    "    \"labels\": \"object\",  # Use 'object' type to store lists\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame with the specified columns and data types\n",
    "df = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in columns.items()})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first model (Eurdem__Defne-llama3.1-8B)\n",
    "### Bbh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt task  score_0  score_1  \\\n",
      "0            Q: not ( True ) and ( True ) is\\n\\n  A:  bbh      1.0      NaN   \n",
      "1       Q: not True or ( False and True ) is\\n\\n  A:  bbh      1.0      NaN   \n",
      "2  Q: not not False and not not not False is\\n\\n  A:  bbh      1.0      NaN   \n",
      "3        Q: ( True and not not not True ) is\\n\\n  A:  bbh      0.0      NaN   \n",
      "4        Q: ( True ) and True or not True is\\n\\n  A:  bbh      0.0      NaN   \n",
      "\n",
      "   score_2  score_3  score_4  score_5  score_6 labels  \n",
      "0      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "1      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "2      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "3      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "4      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_bbh\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_bbh\")]\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            prompt = f'Q: {doc[\"doc\"][\"input\"]}\\n\\n  A:'\n",
    "            score_0 = doc.get(\"acc_norm\", None)\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"bbh\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPQA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 prompt  task  score_0  \\\n",
      "6948  What is the correct answer to this question:A ...  gpqa      0.0   \n",
      "6949  What is the correct answer to this question:We...  gpqa      0.0   \n",
      "6950  What is the correct answer to this question:Th...  gpqa      0.0   \n",
      "6951  What is the correct answer to this question:Wh...  gpqa      1.0   \n",
      "6952  What is the correct answer to this question:Wh...  gpqa      0.0   \n",
      "\n",
      "      score_1  score_2  score_3  score_4  score_5  score_6 labels  \n",
      "6948      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "6949      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "6950      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "6951      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "6952      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_gpqa\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_gpqa\")]\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            prompt = f'What is the correct answer to this question:{doc[\"doc\"][\"Question\"]}\\nChoices:\\n(A) {doc[\"doc\"][\"Correct Answer\"]}\\n(B) {doc[\"doc\"][\"Incorrect Answer 1\"]}\\n(C) {doc[\"doc\"][\"Incorrect Answer 2\"]}\\n(D) {doc[\"doc\"][\"Incorrect Answer 3\"]}\\nAnswer: '\n",
    "            score_0 = doc.get(\"acc_norm\", None)\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"gpqa\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFEval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 prompt    task  score_0  \\\n",
      "7489  Come up with 3 names for a 2B software company...  ifeval      1.0   \n",
      "7490  Write an itinerary for a 10-day trip to Biratn...  ifeval      1.0   \n",
      "7491  Given the sentence \"The dog barked at the cat,...  ifeval      1.0   \n",
      "7492  What is the name of the green-eyed monster tha...  ifeval      0.0   \n",
      "7493  If a + b = 10. And a > b. Is a = 6? Your answe...  ifeval      1.0   \n",
      "\n",
      "      score_1  score_2  score_3  score_4  score_5  score_6 labels  \n",
      "7489      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "7490      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "7491      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "7492      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "7493      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_ifeval\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_ifeval\")]\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            prompt = doc[\"doc\"][\"prompt\"]\n",
    "            score_0 = 1.0 if doc.get(\"prompt_level_strict_acc\", False) else 0.0\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"ifeval\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 prompt  task  score_0  \\\n",
      "8813  Problem:\\nThree unit circles are drawn so they...  math      0.0   \n",
      "8814  Problem:\\nLet $ x$ be a real number such that ...  math      0.0   \n",
      "8815  Problem:\\nIf $0 < \\theta < \\frac{\\pi}{2}$ and ...  math      0.0   \n",
      "8816  Problem:\\nLet $\\mathbf{a},$ $\\mathbf{b},$ and ...  math      0.0   \n",
      "8817  Problem:\\nOne line is defined by\\n\\[\\begin{pma...  math      0.0   \n",
      "\n",
      "      score_1  score_2  score_3  score_4  score_5  score_6 labels  \n",
      "8813      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "8814      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "8815      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "8816      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "8817      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_math\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_math\")]\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            prompt = \"Problem:\" + \"\\n\" + doc[\"doc\"][\"problem\"] + \"\\n\\n\" + \"Solution:\"\n",
    "            score_0 = doc.get(\"exact_match\", None)\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"math\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMLU Pro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  prompt      task  score_0  \\\n",
      "20845  Consider the nonuniform field E= ya_x + xa_y +...  mmlu_pro      0.0   \n",
      "20846  A 15 hp internal-combustion engine runs at 140...  mmlu_pro      0.0   \n",
      "20847  Estimate the overall heat transfer coefficient...  mmlu_pro      0.0   \n",
      "20848  Water at 340°K and a rate of 4 Kg/hr is requir...  mmlu_pro      1.0   \n",
      "20849  The frequency range of a commercially broadcas...  mmlu_pro      0.0   \n",
      "\n",
      "       score_1  score_2  score_3  score_4  score_5  score_6 labels  \n",
      "20845      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "20846      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "20847      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "20848      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "20849      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_mmlu_pro\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_mmlu_pro\")]\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            prompt = f\"{doc['doc']['question']}\\n\"\n",
    "\n",
    "            for i in range(len(doc[\"doc\"][\"options\"])):\n",
    "                prompt += f\"{string.ascii_uppercase[i]}. {doc['doc']['options'][i]}\\n\"\n",
    "\n",
    "            prompt += \"Answer:\"\n",
    "            score_0 = doc.get(\"acc\", None)\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"mmlu_pro\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  prompt  task  score_0  \\\n",
      "21601  In the modest rehearsal space of the Riverside...  musr      0.0   \n",
      "21602  In the bustling robotics firm where I served a...  musr      0.0   \n",
      "21603  The hum of anticipation filled NASA's Space Fl...  musr      1.0   \n",
      "21604  In the heart of Manhattan's urban jungle, amid...  musr      0.0   \n",
      "21605  In the pulsating heart of Silicon Valley, a bu...  musr      1.0   \n",
      "\n",
      "       score_1  score_2  score_3  score_4  score_5  score_6 labels  \n",
      "21601      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "21602      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "21603      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "21604      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n",
      "21605      NaN      NaN      NaN      NaN      NaN      NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# 获取指定路径下所有以 \"samples_leaderboard_musr\" 开头的文件\n",
    "path = \"/workspace/code/lm-evaluation-harness/outputs/Eurdem__Defne-llama3.1-8B\"  # 替换为你的文件路径\n",
    "files = [f for f in os.listdir(path) if f.startswith(\"samples_leaderboard_musr\")]\n",
    "\n",
    "DOC_TO_TEXT = \"{narrative}\\n\\n\" \"{question}\\n\\n\" \"{choices}\\n\" \"Answer:\"\n",
    "\n",
    "# 读取每个 JSON 文件并添加到 DataFrame 中\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            choices = \"\"\n",
    "            for i, choice in enumerate(ast.literal_eval(doc[\"doc\"][\"choices\"])):\n",
    "                choices += f\"{i+1} - {choice}\\n\"\n",
    "            prompt = DOC_TO_TEXT.format(\n",
    "                narrative=doc[\"doc\"][\"narrative\"], question=doc[\"doc\"][\"question\"], choices=choices\n",
    "            )\n",
    "            score_0 = doc.get(\"acc_norm\", None)\n",
    "\n",
    "            # 将数据添加到 DataFrame 中\n",
    "            df = pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    pd.DataFrame(\n",
    "                        {\"prompt\": [prompt], \"task\": \"musr\", \"score_0\": [score_0]}\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "\n",
    "# 显示最终的 DataFrame\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models\n",
    "Locutusque__Hercules-6.1-Llama-3.1-8B, Nekochu__Llama-3.1-8B-German-ORPO, VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct,ValiantLabs__Llama3.1-8B-ShiningValiant2, arcee-ai__Llama-Spark, meta-llama__Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/workspace/code/lm-evaluation-harness/outputs\"\n",
    "\n",
    "# 遍历其他6个文件夹\n",
    "folders = [\n",
    "    \"Locutusque__Hercules-6.1-Llama-3.1-8B\",\n",
    "    \"Nekochu__Llama-3.1-8B-German-ORPO\",\n",
    "    \"VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct\",\n",
    "    \"ValiantLabs__Llama3.1-8B-ShiningValiant2\",\n",
    "    \"arcee-ai__Llama-Spark\",\n",
    "    \"meta-llama__Llama-3.1-8B-Instruct\",\n",
    "]\n",
    "\n",
    "# 定义文件夹对应的列\n",
    "folder_score_columns = {\n",
    "    \"Locutusque__Hercules-6.1-Llama-3.1-8B\": \"score_1\",\n",
    "    \"Nekochu__Llama-3.1-8B-German-ORPO\": \"score_2\",\n",
    "    \"VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct\": \"score_3\",\n",
    "    \"ValiantLabs__Llama3.1-8B-ShiningValiant2\": \"score_4\",\n",
    "    \"arcee-ai__Llama-Spark\": \"score_5\",\n",
    "    \"meta-llama__Llama-3.1-8B-Instruct\": \"score_6\",\n",
    "}\n",
    "\n",
    "# 遍历每个文件夹并读取数据\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_bbh\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_bbh\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    prompt = f'Q: {doc[\"doc\"][\"input\"]}\\n\\n  A:'\n",
    "                    score = doc.get(\"acc_norm\", None)\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n",
    "\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_gpqa\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_gpqa\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    prompt = f'What is the correct answer to this question:{doc[\"doc\"][\"Question\"]}\\nChoices:\\n(A) {doc[\"doc\"][\"Correct Answer\"]}\\n(B) {doc[\"doc\"][\"Incorrect Answer 1\"]}\\n(C) {doc[\"doc\"][\"Incorrect Answer 2\"]}\\n(D) {doc[\"doc\"][\"Incorrect Answer 3\"]}\\nAnswer: '\n",
    "                    score = doc.get(\"acc_norm\", None)\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n",
    "\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_ifeval\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_ifeval\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    prompt = doc[\"doc\"][\"prompt\"]\n",
    "                    score = 1.0 if doc.get(\"prompt_level_strict_acc\", False) else 0.0\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n",
    "\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_math\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_math\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    prompt = \"Problem:\" + \"\\n\" + doc[\"doc\"][\"problem\"] + \"\\n\\n\" + \"Solution:\"\n",
    "                    score = doc.get(\"exact_match\", None)\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n",
    "\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_mmlu_pro\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_mmlu_pro\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    prompt = f\"{doc['doc']['question']}\\n\"\n",
    "\n",
    "                    for i in range(len(doc[\"doc\"][\"options\"])):\n",
    "                        prompt += f\"{string.ascii_uppercase[i]}. {doc['doc']['options'][i]}\\n\"\n",
    "\n",
    "                    prompt += \"Answer:\"\n",
    "                    score = doc.get(\"acc\", None)\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n",
    "\n",
    "        # 获取文件夹下所有以 \"samples_leaderboard_musr\" 开头的文件\n",
    "        files = [\n",
    "            f\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.startswith(\"samples_leaderboard_musr\")\n",
    "        ]\n",
    "        score_column = folder_score_columns[folder]\n",
    "\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    doc = json.loads(line)\n",
    "                    choices = \"\"\n",
    "                    for i, choice in enumerate(ast.literal_eval(doc[\"doc\"][\"choices\"])):\n",
    "                        choices += f\"{i+1} - {choice}\\n\"\n",
    "                    prompt = DOC_TO_TEXT.format(\n",
    "                        narrative=doc[\"doc\"][\"narrative\"], question=doc[\"doc\"][\"question\"], choices=choices\n",
    "                    )\n",
    "                    score = doc.get(\"acc_norm\", None)\n",
    "\n",
    "                    # 查找匹配的 prompt 行并更新对应的 score 列\n",
    "                    df.loc[df[\"prompt\"] == prompt, score_column] = score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\n",
    "    [\"score_0\", \"score_1\", \"score_2\", \"score_3\", \"score_4\", \"score_5\", \"score_6\"]\n",
    "].apply(\n",
    "    lambda row: row[row == row.max()]\n",
    "    .index.map(lambda x: int(x.split(\"_\")[1]))\n",
    "    .tolist(),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>task</th>\n",
       "      <th>score_0</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>score_5</th>\n",
       "      <th>score_6</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>In the modest rehearsal space of the Riverside...</td>\n",
       "      <td>musr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>In the bustling robotics firm where I served a...</td>\n",
       "      <td>musr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>The hum of anticipation filled NASA's Space Fl...</td>\n",
       "      <td>musr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>In the heart of Manhattan's urban jungle, amid...</td>\n",
       "      <td>musr</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>In the pulsating heart of Silicon Valley, a bu...</td>\n",
       "      <td>musr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  prompt  task  score_0  \\\n",
       "21601  In the modest rehearsal space of the Riverside...  musr      0.0   \n",
       "21602  In the bustling robotics firm where I served a...  musr      0.0   \n",
       "21603  The hum of anticipation filled NASA's Space Fl...  musr      1.0   \n",
       "21604  In the heart of Manhattan's urban jungle, amid...  musr      0.0   \n",
       "21605  In the pulsating heart of Silicon Valley, a bu...  musr      1.0   \n",
       "\n",
       "       score_1  score_2  score_3  score_4  score_5  score_6  \\\n",
       "21601      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "21602      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "21603      1.0      1.0      1.0      1.0      0.0      1.0   \n",
       "21604      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "21605      1.0      1.0      1.0      1.0      1.0      1.0   \n",
       "\n",
       "                      labels  \n",
       "21601                    [2]  \n",
       "21602  [0, 1, 2, 3, 4, 5, 6]  \n",
       "21603     [0, 1, 2, 3, 4, 6]  \n",
       "21604  [0, 1, 2, 3, 4, 5, 6]  \n",
       "21605  [0, 1, 2, 3, 4, 5, 6]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt      task  score_0  \\\n",
      "0  Q: Which of the following is a humorous edit o...       bbh      1.0   \n",
      "1   Kant's Universal Law formulation does not ref...  mmlu_pro      1.0   \n",
      "2  Let G_n(s) be the probability generating funct...  mmlu_pro      0.0   \n",
      "3  What is the correct answer to this question:A ...      gpqa      0.0   \n",
      "4  Q: This SVG path element <path d=\"M 25.00,38.0...       bbh      1.0   \n",
      "\n",
      "   score_1  score_2  score_3  score_4  score_5  score_6                 labels  \n",
      "0      1.0      1.0      1.0      0.0      1.0      1.0     [0, 1, 2, 3, 5, 6]  \n",
      "1      1.0      1.0      1.0      1.0      1.0      1.0  [0, 1, 2, 3, 4, 5, 6]  \n",
      "2      0.0      0.0      0.0      1.0      0.0      0.0                    [4]  \n",
      "3      1.0      0.0      0.0      0.0      0.0      1.0                 [1, 6]  \n",
      "4      1.0      1.0      1.0      1.0      1.0      1.0  [0, 1, 2, 3, 4, 5, 6]  \n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=85).reset_index(drop=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[: int(0.8 * len(df))]\n",
    "test_df = df.iloc[int(0.8 * len(df)) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/workspace/code/adapter_router/train.csv\", index=False)\n",
    "test_df.to_csv(\"/workspace/code/adapter_router/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"bbh\", \"gpqa\", \"ifeval\", \"math\", \"mmlu_pro\", \"musr\"]\n",
    "models = [\n",
    "    \"Eurdem__Defne-llama3.1-8B\",\n",
    "    \"Locutusque__Hercules-6.1-Llama-3.1-8B\",\n",
    "    \"Nekochu__Llama-3.1-8B-German-ORPO\",\n",
    "    \"VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct\",\n",
    "    \"ValiantLabs__Llama3.1-8B-ShiningValiant2\",\n",
    "    \"arcee-ai__Llama-Spark\",\n",
    "    \"meta-llama__Llama-3.1-8B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_map = {}\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    accuracy = []\n",
    "    for task in tasks:\n",
    "        task_df = train_df[train_df[\"task\"] == task]\n",
    "        mean_score = task_df[f\"score_{idx}\"].mean()\n",
    "        accuracy.append(mean_score)\n",
    "    accuracy_map[model] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Eurdem__Defne-llama3.1-8B': [0.5310404499242916,\n",
       "  0.32421052631578945,\n",
       "  0.4091954022988506,\n",
       "  0.1456400742115028,\n",
       "  0.3882045929018789,\n",
       "  0.44660194174757284],\n",
       " 'Locutusque__Hercules-6.1-Llama-3.1-8B': [0.5109236426562838,\n",
       "  0.3263157894736842,\n",
       "  0.47126436781609193,\n",
       "  0.137291280148423,\n",
       "  0.3764091858037578,\n",
       "  0.41262135922330095],\n",
       " 'Nekochu__Llama-3.1-8B-German-ORPO': [0.5009733939000649,\n",
       "  0.33157894736842103,\n",
       "  0.41379310344827586,\n",
       "  0.0,\n",
       "  0.33799582463465555,\n",
       "  0.47572815533980584],\n",
       " 'VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct': [0.518494484101233,\n",
       "  0.33157894736842103,\n",
       "  0.45517241379310347,\n",
       "  0.15584415584415584,\n",
       "  0.3973903966597077,\n",
       "  0.4627831715210356],\n",
       " 'ValiantLabs__Llama3.1-8B-ShiningValiant2': [0.44083928185161153,\n",
       "  0.32105263157894737,\n",
       "  0.18160919540229886,\n",
       "  0.08534322820037106,\n",
       "  0.3174321503131524,\n",
       "  0.4255663430420712],\n",
       " 'arcee-ai__Llama-Spark': [0.5230369889682025,\n",
       "  0.3178947368421053,\n",
       "  0.43448275862068964,\n",
       "  0.13172541743970315,\n",
       "  0.39018789144050103,\n",
       "  0.42880258899676377],\n",
       " 'meta-llama__Llama-3.1-8B-Instruct': [0.5122215011897037,\n",
       "  0.3494736842105263,\n",
       "  0.41839080459770117,\n",
       "  0.137291280148423,\n",
       "  0.37995824634655534,\n",
       "  0.40938511326860844]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task\n",
       "mmlu_pro    2452\n",
       "bbh         1138\n",
       "math         246\n",
       "gpqa         242\n",
       "musr         138\n",
       "ifeval       106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计task的数量\n",
    "test_df[\"task\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
